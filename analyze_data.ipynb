{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c2a1df",
   "metadata": {},
   "source": [
    "# On Cloud N: Cloud Cover Detection Challenge\n",
    "The [DrivenData](https://www.drivendata.org/competitions/83/cloud-cover/) challenge \n",
    "\n",
    "Thanks a lot to https://www.drivendata.co/blog/cloud-cover-benchmark/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e91a1c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Satellite imagery is critical for a wide variety of applications from disaster management and recovery, to agriculture, to military intelligence. Clouds present a major obstacle for all of these use cases, and usually have to be identified and removed from a dataset before satellite imagery can be used. Improving methods of identifying clouds can unlock the potential of an unlimited range of satellite imagery use cases, enabling faster, more efficient, and more accurate image-based research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da5b022",
   "metadata": {},
   "source": [
    "The goal is to detect (to label) clouds in satellite imagery to remove cloud interference. The challenge uses publicly available satellite data from the [Sentinel-2 mission](https://sentinel.esa.int/web/sentinel/missions/sentinel-2), which captures wide-swath, high-resolution, multi-spectral imaging. For each tile, data is separated into different bands of light across the full visible spectrum, near-infrared, and infrared light. Data is shared through [Microsoft's Planetary Computer](https://planetarycomputer.microsoft.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f9d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_path import path\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e65eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.cwd().parent.resolve() / 'drivenData_clouds/'\n",
    "TRAIN_FEATURES = DATA_DIR / 'data/train_features'\n",
    "TRAIN_LABELS = DATA_DIR / 'data/train_labels'\n",
    "\n",
    "assert TRAIN_FEATURES.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3167244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloudpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adwu</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adwz</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adxp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adxp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeaj</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeaj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chip_id  location              datetime                   cloudpath\n",
       "0    adwp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwp\n",
       "1    adwu  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwu\n",
       "2    adwz  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwz\n",
       "3    adxp  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adxp\n",
       "4    aeaj  Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeaj"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BANDS = ['B02', 'B03', 'B04', 'B08'] #different range of wavelengths (aka bands) blue/green/red/infrared\n",
    "\n",
    "# Metadata\n",
    "train_meta = pd.read_csv(DATA_DIR / 'train_metadata.csv')\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd562657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chip_id     11748\n",
       "location       81\n",
       "datetime       91\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta[['chip_id', 'location', 'datetime']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8b31b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graph of locations x number of chips\n",
    "train_location_counts = (train_meta.groupby('location')['chip_id'].nunique().sort_values(ascending=False))\n",
    "plt.figure(figsize=(12,4))\n",
    "train_location_counts.head(25).plot(kind='bar', color='lightgray')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Location')\n",
    "plt.ylabel('Number of chips')\n",
    "plt.title('Number of Train chips by Location (Top 25)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db76f128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>1015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>10407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      chip_count\n",
       "year            \n",
       "2018         326\n",
       "2019        1015\n",
       "2020       10407"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time range\n",
    "train_meta['datetime'] = pd.to_datetime(train_meta['datetime'])\n",
    "train_meta['year'] = train_meta.datetime.dt.year\n",
    "train_meta.groupby('year')[['chip_id']].nunique().sort_index().rename(columns={'chip_id' : 'chip_count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e5c6e",
   "metadata": {},
   "source": [
    "We can read this from problem description but to be sure let's find out the date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e1cc3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2018-03-07 08:46:02+0000', tz='UTC'),\n",
       " Timestamp('2020-09-14 08:28:49+0000', tz='UTC'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta['datetime'].min(), train_meta['datetime'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26433db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>chip_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>South America - Brazil</th>\n",
       "      <th>2020-09-06 15:02:37+00:00</th>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Port Gentil</th>\n",
       "      <th>2020-09-08 09:50:58+00:00</th>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uganda</th>\n",
       "      <th>2019-04-25 08:29:37+00:00</th>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Australia - Central</th>\n",
       "      <th>2020-08-11 01:24:00+00:00</th>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malabo</th>\n",
       "      <th>2020-09-06 10:00:03+00:00</th>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jimma</th>\n",
       "      <th>2020-05-31 08:07:58+00:00</th>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chifunfu</th>\n",
       "      <th>2020-04-29 08:20:47+00:00</th>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South America - Suriname</th>\n",
       "      <th>2020-06-03 14:11:18+00:00</th>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Isiro</th>\n",
       "      <th>2020-08-28 08:39:29+00:00</th>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pibor</th>\n",
       "      <th>2020-08-17 08:18:22+00:00</th>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    chip_count\n",
       "location                 datetime                             \n",
       "South America - Brazil   2020-09-06 15:02:37+00:00         261\n",
       "Port Gentil              2020-09-08 09:50:58+00:00         223\n",
       "Uganda                   2019-04-25 08:29:37+00:00         220\n",
       "Australia - Central      2020-08-11 01:24:00+00:00         209\n",
       "Malabo                   2020-09-06 10:00:03+00:00         206\n",
       "Jimma                    2020-05-31 08:07:58+00:00         201\n",
       "Chifunfu                 2020-04-29 08:20:47+00:00         197\n",
       "South America - Suriname 2020-06-03 14:11:18+00:00         197\n",
       "Isiro                    2020-08-28 08:39:29+00:00         197\n",
       "Pibor                    2020-08-17 08:18:22+00:00         197"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chips_per_locationtime = (\n",
    "    train_meta.groupby(['location','datetime'])[['chip_id']]\n",
    "    .nunique()\n",
    "    .sort_values(by='chip_id', ascending=False)\n",
    "    .rename(columns={'chip_id' : 'chip_count'})\n",
    ")\n",
    "\n",
    "chips_per_locationtime.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20831b",
   "metadata": {},
   "source": [
    "## Images\n",
    "Images are stored as GeoTiffs (512x512 resolution, one pixel is about 10m) covering the exact same area.\n",
    "\n",
    "Sentinel-2 flies over the part of the Earth between 56° South (Cape Horn, South America) and 82.8° North (above Greenland), so our observations are all between these two latitudes. The chips are mostly from Africa, South America, and Australia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20f598d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not Series",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3124/2684425220.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mtrain_meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRAIN_FEATURES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRAIN_LABELS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mtrain_meta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3124/2684425220.py\u001b[0m in \u001b[0;36madd_paths\u001b[1;34m(df, feature_dir, label_dir, bands)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mband\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbands\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_dir\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"chip_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;31m#df[f\"{band}_path\"] = feature_dir / df[\"chip_id\"] / f\"{band}.tif\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"{band}_path\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drivenData3\\lib\\pathlib.py\u001b[0m in \u001b[0;36m__truediv__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__truediv__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_child\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    926\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__rtruediv__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drivenData3\\lib\\pathlib.py\u001b[0m in \u001b[0;36m_make_child\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_child\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m         \u001b[0mdrv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m         drv, root, parts = self._flavour.join_parsed_parts(\n\u001b[0;32m    706\u001b[0m             self._drv, self._root, self._parts, drv, root, parts)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drivenData3\\lib\\pathlib.py\u001b[0m in \u001b[0;36m_parse_args\u001b[1;34m(cls, args)\u001b[0m\n\u001b[0;32m    656\u001b[0m                 \u001b[0mparts\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m                 \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m                     \u001b[1;31m# Force-cast str subclasses to str (issue #21127)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not Series"
     ]
    }
   ],
   "source": [
    "def add_paths(df, feature_dir, label_dir=None, bands=BANDS):\n",
    "    \"\"\"\n",
    "    Given dataframe with a column for chip_id, returns a dataframe with a column\n",
    "    added indicating the path to each band's TIF image as \"{band}_path\", eg \"B02_path\".\n",
    "    A column is also added to the dataframe with paths to the label TIF, if the\n",
    "    path to the labels directory is provided.\n",
    "    \"\"\"\n",
    "    for band in bands:\n",
    "        print(feature_dir / df[\"chip_id\"])\n",
    "        #df[f\"{band}_path\"] = feature_dir / df[\"chip_id\"] / f\"{band}.tif\"\n",
    "        assert df[f\"{band}_path\"].path.exists().all()\n",
    "    if label_dir is not None:\n",
    "        df[\"label_path\"] = label_dir / (df[\"chip_id\"] + \".tif\")\n",
    "        assert df[\"label_path\"].path.exists().all()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_meta = add_paths(train_meta, TRAIN_FEATURES, TRAIN_LABELS)\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd64849",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_chip = train_meta[train_meta[\"chip_id\"] == \"pbyl\"]\n",
    "\n",
    "display(example_chip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ddaa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "example_chip = example_chip.iloc[0]\n",
    "with rasterio.open(example_chip[\"B04_path\"]) as img:\n",
    "    chip_metadata = img.meta\n",
    "    img_array = img.read(1)\n",
    "\n",
    "chip_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de019af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the image array look like?\n",
    "print(\"Image array shape:\", img_array.shape)\n",
    "img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac54fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(img_array).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400413f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_array)\n",
    "plt.title(f\"B04 band for chip id {example_chip.chip_id}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb90c06",
   "metadata": {},
   "source": [
    "### Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# longitude/latitude of image's center\n",
    "with rasterio.open(example_chip[\"B04_path\"]) as img:\n",
    "    lon, lat = img.lnglat()\n",
    "    bounds = img.bounds\n",
    "print(f\"Longitude: {lon}, latitude: {lat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee62b79",
   "metadata": {},
   "source": [
    "There are different projections (crs atribute in metadata) in BoundingBox and Long/Lat. We can transform them with **pyproj** library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bdac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84892eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_long_bounds(filepath):\n",
    "    \"\"\"Given the path to a GeoTIFF, returns the image bounds in latitude and\n",
    "    longitude coordinates.\n",
    "\n",
    "    Returns points as a tuple of (left, bottom, right, top)\n",
    "    \"\"\"\n",
    "    with rasterio.open(filepath) as im:\n",
    "        bounds = im.bounds\n",
    "        meta = im.meta\n",
    "    # create a converter starting with the current projection\n",
    "    current_crs = pyproj.CRS(meta[\"crs\"])\n",
    "    crs_transform = pyproj.Transformer.from_crs(current_crs, current_crs.geodetic_crs)\n",
    "\n",
    "    # returns left, bottom, right, top\n",
    "    return crs_transform.transform_bounds(*bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "left, bottom, right, top = lat_long_bounds(example_chip[\"B04_path\"])\n",
    "print(\n",
    "    f\"Image coordinates (lat, long):\\nStart: ({left}, {bottom})\"\n",
    "    f\"\\nEnd: ({right}, {top})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701820c",
   "metadata": {},
   "source": [
    "### True color image\n",
    "We can also make a composition image from all three visible bands (blue/green/red) to visualize a high-quality, true color image. To show this image, we'll use the xarraz and xrspatial packages designed for Sentinel-2 satellite data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691264f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "import xrspatial.multispectral as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7778c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xarray(filepath):\n",
    "    \"\"\"Put images in xarray.DataArray format\"\"\"\n",
    "    im_arr = np.array(Image.open(filepath))\n",
    "    return xarray.DataArray(im_arr, dims=[\"y\", \"x\"])\n",
    "\n",
    "\n",
    "def true_color_img(chip_id, data_dir=TRAIN_FEATURES):\n",
    "    \"\"\"Given the path to the directory of Sentinel-2 chip feature images,\n",
    "    plots the true color image\"\"\"\n",
    "    chip_dir = data_dir / chip_id\n",
    "    red = get_xarray(chip_dir / \"B04.tif\")\n",
    "    green = get_xarray(chip_dir / \"B03.tif\")\n",
    "    blue = get_xarray(chip_dir / \"B02.tif\")\n",
    "\n",
    "    return ms.true_color(r=red, g=green, b=blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb1a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "im = true_color_img(example_chip.chip_id)\n",
    "ax.imshow(im)\n",
    "plt.title(f\"True color image for chip id {example_chip.chip_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2758ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_random_chip(random_state):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    random_chip = train_meta.sample(random_state=random_state).iloc[0]\n",
    "    ax[0].imshow(true_color_img(random_chip.chip_id))\n",
    "    ax[0].set_title(f\"Chip {random_chip.chip_id}\\n(Location: {random_chip.location})\")\n",
    "    label_im = Image.open(random_chip.label_path)\n",
    "    ax[1].imshow(label_im)\n",
    "    ax[1].set_title(f\"Chip {random_chip.chip_id} label\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e355f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_chip(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb1f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_chip(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4da300",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_chip(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ec624",
   "metadata": {},
   "source": [
    "### Split the Data\n",
    "We split data into 1/3 validation and 2/3 training sets. \n",
    "\n",
    "There are some posibilities to change. Like changing splitting ratio (20/80 for example) or not to split by chips but by location. We can try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c24ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91536fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3877, 5), (7871, 5))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(9) # set a seed for reproducibility\n",
    "\n",
    "# put 1/3 of chips into the validation set\n",
    "chip_ids = train_meta.chip_id.unique().tolist()\n",
    "val_chip_ids = random.sample(chip_ids, round(len(chip_ids) * 0.33))\n",
    "\n",
    "val_mask = train_meta.chip_id.isin(val_chip_ids)\n",
    "val = train_meta[val_mask].copy().reset_index(drop=True)\n",
    "train = train_meta[~val_mask].copy().reset_index(drop=True)\n",
    "\n",
    "val.shape, train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2df5d968",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['B02_path', 'B03_path', 'B04_path', 'B08_path'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3124/2836728570.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfeature_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'chip_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34mf'{band}_path'\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mband\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mBANDS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mval_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mval_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'chip_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drivenData3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3462\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3464\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3466\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drivenData3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drivenData3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1377\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['B02_path', 'B03_path', 'B04_path', 'B08_path'] not in index\""
     ]
    }
   ],
   "source": [
    "# separate features from labels\n",
    "feature_cols = ['chip_id'] + [f'{band}_path' for band in BANDS]\n",
    "\n",
    "val_x = val[feature_cols].copy()\n",
    "val_y = val[['chip_id', 'label_path']].copy()\n",
    "\n",
    "train_x = train[feature_cols].copy()\n",
    "train_y = train[['chip_id', 'label_path']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bec8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3803cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f764c77",
   "metadata": {},
   "source": [
    "## CODE EXECUTION\n",
    "Let's sart with the basic neural network model (more specificaly publicly available convolutional neural network U-Net) using some of the standard libraries (PyTorch Lightning). (There is a lot of room for improvements).\n",
    "\n",
    "U-Net was first designed to help process biomedical imaging and identify things like signs of disease. The basic structure is an encoder network followed by a decoder network. We'll use a pretrained backbone called ResNet34 as our encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80286a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create benchmark_src folder\n",
    "submission_dir = Path(\"benchmark_src\")\n",
    "if submission_dir.exists():\n",
    "    shutil.rmtree(submission_dir)\n",
    "\n",
    "submission_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395971f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file {submission_dir}/cloud_dataset.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "\n",
    "class CloudDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Reads in images, transforms pixel values and serves a\n",
    "    dictionary containing chip ids, image tensors and label\n",
    "    mask (when available)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "    self,\n",
    "    x_paths: pd.DataFrame,\n",
    "    bands: List[str],\n",
    "    y_paths: Optional[pd.DataFrame] = None,\n",
    "    transforms: Optional[list] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudDataset class.\n",
    "        \n",
    "        Args:\n",
    "            x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be \n",
    "                a column for chip_id and a column with the path to the TIF for each of bands\n",
    "            bands (list[str]): list of the bands included in the data\n",
    "            y_paths (pd.DataFrame, optional): a dataframe with a row for each chip and columns for chip_id\n",
    "                and the path to the label TIF with ground truth cloud cover\n",
    "            transforms (list, optional): list of transforms to apply to the feature data (eg augmentations)\n",
    "        \"\"\"\n",
    "        self.data = x_paths\n",
    "        self.label = y_paths\n",
    "        self.transforms = transforms\n",
    "        self.bands = bands\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        # Loads an n-channel image from a chip-level dataframe\n",
    "        img = self.data.loc[idx]\n",
    "        band_arrs = []\n",
    "        for band in self.bands:\n",
    "            with rasterio.open(img[f\"{band}_path\"]) as b:\n",
    "                band_arr = b.read(1).astype(\"float32\")\n",
    "            band_arrs.append(bamd_arr)\n",
    "        x_arr = np.stack(band_arrs, axis = -1)\n",
    "        \n",
    "        # Apply data augmentation, if provided\n",
    "        if self.transforms:\n",
    "            x_arr = self.transforms(image=x_arr)[\"image\"]\n",
    "        x_arr = np.transpose(x_arr, [2,0,1])\n",
    "        \n",
    "        # Prepare dictionary for item\n",
    "        item = {\"chip_id\" : img.chip_id, \"chip\": x_arr}\n",
    "        \n",
    "        # Load label if available\n",
    "        if self.label is not None:\n",
    "            label_path = self.label.loc[idx].label_path\n",
    "            with rasterio.open(label_path) as lp:\n",
    "                y_arr = lp.read(1).astype(\"float32\")\n",
    "            # Apply same data augmentation to the label\n",
    "            if self.transforms:\n",
    "                y_arr = self.transforms(image=y_arr)[\"image\"]\n",
    "            item[\"label\"] = y_arr\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c328f44a",
   "metadata": {},
   "source": [
    "## CloudModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b535b020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing {submission_dir}/cloud_model.py\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '{submission_dir}/cloud_model.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3124/2953236333.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'file'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'{submission_dir}/cloud_model.py'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'from typing import Optional, List\\n\\nimport pandas as pd\\nimport pytorch_lightning as pl\\nimport segmentation_models_pytorch as smp\\nimport torch\\n\\ntry:\\n    from cloud_dataset import CloudDataset\\n    from losses import intersection_over_union\\nexcept ImportError:\\n    from benchmark_src.cloud_dataset import CloudDataset\\n    from benchmark_src.losses import intersection_over_union\\n    \\nclass CloudModel(pl.LighningModule):\\n    \\n    def __init__(\\n        self,\\n        bands: List[str],\\n        x_train: Optional[pd.DataFrame] = None,\\n        y_train: Optional[pd.DataFrame] = None,\\n        x_val: Optional[pd.DataFrame] = None,\\n        y_val: Optional[pd.DataFrame] = None,\\n        hparams: dict = {},\\n    ):\\n        \"\"\"\\n        Instantiate the CloudModel class based on the pl.LightningModule\\n        (https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html).\\n\\n        Args:\\n            bands (list[str]): Names of the bands provided for each chip\\n            x_train (pd.DataFrame, optional): a dataframe of the training features with a row for each chip.\\n                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\\n                Required for model training\\n            y_train (pd.DataFrame, optional): a dataframe of the training labels with a for each chip\\n                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\\n                Required for model training\\n            x_val (pd.DataFrame, optional): a dataframe of the validation features with a row for each chip.\\n                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\\n                Required for model training\\n            y_val (pd.DataFrame, optional): a dataframe of the validation labels with a for each chip\\n                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\\n                Required for model training\\n            hparams (dict, optional): Dictionary of additional modeling parameters.        \\n        \"\"\"\\n        super().__init__()\\n        self.hparams.update(hparams)\\n        self.save_hyperparameters()\\n        \\n        # required\\n        self.bands = bands\\n        \\n        # optional modeling params\\n        self.backbone = self.hparams.get(\"backbone\", \"resnet34\")\\n        self.weights = self.hparams.get(\"weights\", \"imagenet\")\\n        self.learning_rate = self.hparams.get(\"lr\", 1e-3)\\n        self.patience = self.hparams.get(\"patience\", 4)\\n        self.num_workers = self.hparams.get(\"num_workers\", 2)\\n        self.batch_size = self.hparmas.get(\"batch_size\", 32)\\n        self.gpu = self.hparams.get(\"gpu\", False)\\n        self.transform = None\\n        \\n        # Instantiante datasets, model, and trainer params if provided\\n        self.train_dataset = CloudDatset(\\n            x_paths = x_train,\\n            bands = self.bands,\\n            y_paths = y_train,\\n            transforms = self.transform\\n        )\\n        \\n        self.val_dataset = CloudDataset(\\n            x_path = v_val,\\n            bands = self.bands,\\n            y_paths = y_val,\\n            transforms = None\\n        )\\n        \\n        self.model = self._prepare_model()\\n        \\n    ## Required LightningModule methods ##\\n    \\n    def forward(self, image: torch.Tensor):\\n        # Forward pass\\n        return self.model(image)\\n    \\n    def training_step(self, batch: dict, batch_idx: int):\\n        \"\"\"\\n        Training step.\\n\\n        Args:\\n            batch (dict): dictionary of items from CloudDataset of the form\\n                {\\'chip_id\\': list[str], \\'chip\\': list[torch.Tensor], \\'label\\': list[torch.Tensor]}\\n            batch_idx (int): batch number        \\n        \"\"\"\\n        if self.train_dataset.data is None:\\n            raise ValueError(\\n                \"x_train and y_train mus be specified when CloudModel is instantiated to run training\"\\n            )\\n        \\n        # Switch on training mode\\n        self.model.train()\\n        torch.set_grad_enabled(True)\\n        \\n        # Load images and labels\\n        x = batch[\"chip\"]\\n        y = batch[\"label\"].long()\\n        if self.gpu:\\n            x, y = x.cuda(non_blocking = True), y.cuda(non_blocking=True)\\n            \\n        # Forward pass & softmax\\n        preds = self.forward(x)\\n        preds = torch.softmax(preds, dim=1)[:, 1]\\n        preds = (preds > 0.5) * 1 #convert to int\\n        \\n        # Log batch IOU\\n        batch_iou = intersection_over_union(preds, y)\\n        self.log(\\n            \"iou\", batch_iou, on_step=True, on_epoch=True, prog_bar=True, logger=True\\n        )\\n        return batch_iou\\n    \\n    def train_dataloader(self):\\n        #Dataloader class for training\\n        return torch.utils.data.DataLoader(\\n            self.train_dataset,\\n            batch_size = self.batch_size,\\n            num_workers = self.num_workers,\\n            shuffle = True,\\n            pin_memory = True,\\n        )\\n    \\n    def configure_optimizers(self):\\n        opt = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\\n        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\\n        return [opt], [sch]\\n    \\n    ## Convenience Methods ##\\n    \\n    def _prepare_model(self):\\n        # Instantiate U-Net model\\n        unet_model = smp.Unet(\\n            encoder_name = self.backbone,\\n            encoder_wights = self.weights,\\n            in_channels = 4,\\n            classes = 2,\\n        )\\n        \\n        if self.gpu:\\n            unet_model.cuda()\\n            \\n        return unet_model\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\drivenData3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2404\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2405\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2406\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2407\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drivenData3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[0margs_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic_params\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drivenData3\\lib\\site-packages\\decorator.py\u001b[0m in \u001b[0;36mfun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drivenData3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drivenData3\\lib\\site-packages\\IPython\\core\\magics\\osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m         \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'a'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    857\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '{submission_dir}/cloud_model.py'"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/cloud_model.py\n",
    "from typing import Optional, List\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from losses import intersection_over_union\n",
    "except ImportError:\n",
    "    from benchmark_src.cloud_dataset import CloudDataset\n",
    "    from benchmark_src.losses import intersection_over_union\n",
    "    \n",
    "class CloudModel(pl.LighningModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        bands: List[str],\n",
    "        x_train: Optional[pd.DataFrame] = None,\n",
    "        y_train: Optional[pd.DataFrame] = None,\n",
    "        x_val: Optional[pd.DataFrame] = None,\n",
    "        y_val: Optional[pd.DataFrame] = None,\n",
    "        hparams: dict = {},\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudModel class based on the pl.LightningModule\n",
    "        (https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html).\n",
    "\n",
    "        Args:\n",
    "            bands (list[str]): Names of the bands provided for each chip\n",
    "            x_train (pd.DataFrame, optional): a dataframe of the training features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_train (pd.DataFrame, optional): a dataframe of the training labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            x_val (pd.DataFrame, optional): a dataframe of the validation features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_val (pd.DataFrame, optional): a dataframe of the validation labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            hparams (dict, optional): Dictionary of additional modeling parameters.        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams)\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # required\n",
    "        self.bands = bands\n",
    "        \n",
    "        # optional modeling params\n",
    "        self.backbone = self.hparams.get(\"backbone\", \"resnet34\")\n",
    "        self.weights = self.hparams.get(\"weights\", \"imagenet\")\n",
    "        self.learning_rate = self.hparams.get(\"lr\", 1e-3)\n",
    "        self.patience = self.hparams.get(\"patience\", 4)\n",
    "        self.num_workers = self.hparams.get(\"num_workers\", 2)\n",
    "        self.batch_size = self.hparmas.get(\"batch_size\", 32)\n",
    "        self.gpu = self.hparams.get(\"gpu\", False)\n",
    "        self.transform = None\n",
    "        \n",
    "        # Instantiante datasets, model, and trainer params if provided\n",
    "        self.train_dataset = CloudDatset(\n",
    "            x_paths = x_train,\n",
    "            bands = self.bands,\n",
    "            y_paths = y_train,\n",
    "            transforms = self.transform\n",
    "        )\n",
    "        \n",
    "        self.val_dataset = CloudDataset(\n",
    "            x_path = v_val,\n",
    "            bands = self.bands,\n",
    "            y_paths = y_val,\n",
    "            transforms = None\n",
    "        )\n",
    "        \n",
    "        self.model = self._prepare_model()\n",
    "        \n",
    "    ## Required LightningModule methods ##\n",
    "    \n",
    "    def forward(self, image: torch.Tensor):\n",
    "        # Forward pass\n",
    "        return self.model(image)\n",
    "    \n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number        \n",
    "        \"\"\"\n",
    "        if self.train_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_train and y_train mus be specified when CloudModel is instantiated to run training\"\n",
    "            )\n",
    "        \n",
    "        # Switch on training mode\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking = True), y.cuda(non_blocking=True)\n",
    "            \n",
    "        # Forward pass & softmax\n",
    "        preds = self.forward(x)\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5) * 1 #convert to int\n",
    "        \n",
    "        # Log batch IOU\n",
    "        batch_iou = intersection_over_union(preds, y)\n",
    "        self.log(\n",
    "            \"iou\", batch_iou, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return batch_iou\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        #Dataloader class for training\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size = self.batch_size,\n",
    "            num_workers = self.num_workers,\n",
    "            shuffle = True,\n",
    "            pin_memory = True,\n",
    "        )\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
    "        return [opt], [sch]\n",
    "    \n",
    "    ## Convenience Methods ##\n",
    "    \n",
    "    def _prepare_model(self):\n",
    "        # Instantiate U-Net model\n",
    "        unet_model = smp.Unet(\n",
    "            encoder_name = self.backbone,\n",
    "            encoder_wights = self.weights,\n",
    "            in_channels = 4,\n",
    "            classes = 2,\n",
    "        )\n",
    "        \n",
    "        if self.gpu:\n",
    "            unet_model.cuda()\n",
    "            \n",
    "        return unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c06ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_src.cloud_model import CloudModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5eac52",
   "metadata": {},
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ec42147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8952e91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CloudModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3124/102686860.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Set up pytorch_lightning.Trainer object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m cloud_model = CloudModel(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mbands\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBANDS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CloudModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up pytorch_lightning.Trainer object\n",
    "cloud_model = CloudModel(\n",
    "    bands=BANDS,\n",
    "    x_train=train_x,\n",
    "    y_train=train_y,\n",
    "    x_val=val_x,\n",
    "    y_val=val_y,\n",
    ")\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='iou_epoch', mode='max', verbose=True)\n",
    "\n",
    "early_stopping_callback = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor='iou_epoch',\n",
    "    patience=(cloud_model.patience * 3),\n",
    "    mode='max',\n",
    "    vertbose=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=None,\n",
    "    fast_dev_run=False,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19637f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=cloud_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e0e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_assets_dir = submission_dir / \"assests\"\n",
    "submission_assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_weight_path = submission_assets_dir / \"cloud_model.pt\"\n",
    "torch.save(cloud_model.state_dict(), model_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file benchmark_src/main.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import typer\n",
    "\n",
    "try:\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from cloud_model import CloudModel\n",
    "except ImportError:\n",
    "    from benchmark_src.cloud_dataset import CloudDataset\n",
    "    from benchmark_src.cloud_model import CloudModel\n",
    "    \n",
    "ROOT_DIRECTORY = Path('/codeexecution')\n",
    "PREDICTIONS_DIRECTORY = ROOT_DIRECTORY / 'predictoins'\n",
    "ASSETS_DIRECTORY = ROOT_DIRECTORY / 'assets'\n",
    "DATA_DIRECTORY = ROOT_DIRECTORY / 'data'\n",
    "INPUT_IMAGES_DIRECTORY = DATA_DIRECTORY / 'test_features'\n",
    "\n",
    "# Set the pytorch cache directory and include cached models in your submission.zip\n",
    "os.environ['TORCH_HOME'] = str(ASSETS_DIRECTORY / 'assets/torch')\n",
    "\n",
    "def get_metadata(features_dir: os.PathLike, bands: List[str]):\n",
    "    \"\"\"\n",
    "    Given a folder of feature data, return a dataframe where the index is the chip id\n",
    "    and there is a column for the path to each band's TIF image.\n",
    "\n",
    "    Args:\n",
    "        features_dir (os.PathLike): path to the directory of feature data, which should have\n",
    "            a folder for each chip\n",
    "        bands (list[str]): list of bands provided for each chip\n",
    "    \"\"\"\n",
    "    chip_metadata = pd.DataFrame(index=[f\"{band}_path\" for band in bands])\n",
    "    chip_ids = (\n",
    "        pth.name for pth in features_dir.iterdir() if not pth.name.startswith('.')\n",
    "    )\n",
    "    \n",
    "    for chip_id in chip_ids:\n",
    "        chip_bands = [features_dir / chip_id / f\"{band}.tif\" for band in bands]\n",
    "        chip_metadata[chip_id] = chip_bands\n",
    "\n",
    "    return chip_metadata.transpose().rest_index().rename(columns={\"index\" : \"chip_id\"})\n",
    "\n",
    "def make_predictoins(\n",
    "    model: CloudModel,\n",
    "    x_paths: pd.DataFrame,\n",
    "    bands: List[str],\n",
    "    predictions_dir: os.PathLike,\n",
    "):\n",
    "    \"\"\"Predicts cloud cover and saves results to the predictions directory.\n",
    "\n",
    "    Args:\n",
    "        model (CloudModel): an instantiated CloudModel based on pl.LightningModule\n",
    "        x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands provided\n",
    "        bands (list[str]): list of bands provided for each chip\n",
    "        predictions_dir (os.PathLike): Destination directory to save the predicted TIF masks\n",
    "    \"\"\"\n",
    "    test_dataset = CloudDataset(x_path=x_paths, bands=bands)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size = model.batch_size,\n",
    "        num_workers = model.num_workers,\n",
    "        shuffle = False,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "    \n",
    "    for batch_index, batch in enumerate(test_dataloader):\n",
    "        logger.debug(f\"Predicting batch {batch_index} of {len(test_dataloader)}\")\n",
    "        x = batch[\"chip\"]\n",
    "        preds = model.forward[x]\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5).detach().numpy().astype('uint8')\n",
    "        for chip_id, pred in zip(batc['chip_id'], preds):\n",
    "            chip_pred_path = prediction_dir / f\"{chip_id}.tif\"\n",
    "            chip_pred_im = Image.formarray(red)\n",
    "            chip_pred_im.save(chip_pred_path)\n",
    "            \n",
    "def main():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c19643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
